{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anushkaghei/Underwater-Image-dehazing/blob/main/Evaluation_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vau_CVoljg0C"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMOX3cjRjg0F"
      },
      "source": [
        "# RGB GRAPHS\n",
        "\n",
        "RGB plots visualize the distribution of pixel intensities in the red, green, and blue channels of an image. They provide insights into the color composition and balance of an image. Often used for qualitative assessment of color distribution and balance in images. They can reveal issues such as color casts or saturation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Open the original image\n",
        "original1 = Image.open('/content/160_im.png') #path for raw image\n",
        "\n",
        "gan1 = Image.open('/content/gen_1.png')# path for generated image\n",
        "\n",
        "# Open the reference image\n",
        "ref1 = Image.open('/content/160_img_.png')\n",
        "\n",
        "# Resize the images to 256x256 pixels\n",
        "original1 = original1.resize((256, 256))\n",
        "gan1 = gan1.resize((256, 256))\n",
        "ref1 = ref1.resize((256, 256))\n",
        "\n",
        "def plot_histogram(image, text):\n",
        "    # Convert image to RGB mode\n",
        "    image = image.convert('RGB')\n",
        "    # Split the R, G and B channels\n",
        "    imageR, imageG, imageB = image.split()\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(text)\n",
        "    plt.imshow(image)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Histogram of image\")\n",
        "    plt.plot(imageR.histogram(), color='red')\n",
        "    plt.plot(imageG.histogram(), color='green')\n",
        "    plt.plot(imageB.histogram(), color='blue')\n",
        "    plt.show()\n",
        "\n",
        "plot_histogram(gan1, \"Generated Image\")# replace title\n",
        "plot_histogram(ref1, \"Reference Image\")\n"
      ],
      "metadata": {
        "id": "7HlL6kqOrVxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWRhSZYcjg0J"
      },
      "source": [
        "# PSNR & MSE (Peak Signal-to-Noise Ratio and Mean Squared Error):\n",
        "\n",
        "PSNR measures the quality of an image by comparing it with a reference image. It quantifies the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. MSE measures the average squared difference between the pixels of the two images.\n",
        "Higher PSNR values indicate higher image fidelity, while lower MSE values indicate better similarity between images."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def psnr(reference, fused):\n",
        "    # Convert images to RGB mode\n",
        "    reference = reference.convert('RGB')\n",
        "    fused = fused.convert('RGB')\n",
        "\n",
        "    # Convert images to NumPy arrays\n",
        "    reference_array = np.array(reference)\n",
        "    fused_array = np.array(fused)\n",
        "\n",
        "    # Calculate PSNR\n",
        "    R2 = np.amax(reference_array)**2\n",
        "    MSE = np.sum(np.power(np.subtract(reference_array, fused_array), 2))\n",
        "    MSE /= (reference_array.shape[0] * reference_array.shape[1])\n",
        "    PSNR = 10 * np.log10(R2 / MSE)\n",
        "\n",
        "    print(\"Reference vs Generated   -\", \"MSE: \", MSE, \"PSNR:\", PSNR)\n",
        "    print('')\n",
        "\n",
        "print(\"MSE & PSNR of 160_img_.png\")\n",
        "psnr(ref1, gan1)\n"
      ],
      "metadata": {
        "id": "1xbQLQVGsYlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTEMWNlTjg0K"
      },
      "source": [
        "# SSIM (Structural Similarity Index):\n",
        "\n",
        "SSIM compares the structural similarity between two images. It considers luminance, contrast, and structure similarity to provide a comprehensive measure of image quality. It is widely used for assessing the perceived quality of images. It is particularly effective in evaluating images subjected to compression or distortion."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import metrics\n",
        "from skimage.metrics import structural_similarity\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "gan = cv2.imread('/content/gen_1.png') # generated image\n",
        "reference = cv2.imread('/content/160_img_.png') #reference image\n",
        "\n",
        "# Resize images\n",
        "gan = cv2.resize(gan, (256, 256))\n",
        "reference = cv2.resize(reference, (256, 256))\n",
        "\n",
        "# Convert images to grayscale\n",
        "gan_gray = cv2.cvtColor(gan, cv2.COLOR_BGR2GRAY)\n",
        "ref_gray = cv2.cvtColor(reference, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Compute SSIM between two images\n",
        "(score, diff) = structural_similarity(gan_gray, ref_gray, full=True)\n",
        "print(\"Image similarity:\", score)\n",
        "\n",
        "# Convert diff to 8-bit unsigned integer in the range [0,255]\n",
        "diff = (diff * 255).astype(\"uint8\")\n",
        "\n",
        "# Threshold the difference image, followed by finding contours to\n",
        "# obtain the regions of the two input images that differ\n",
        "thresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
        "contours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "contours = contours[0] if len(contours) == 2 else contours[1]\n",
        "\n",
        "mask = np.zeros(gan.shape, dtype='uint8')\n",
        "filled_after = reference.copy()\n",
        "\n",
        "for c in contours:\n",
        "    area = cv2.contourArea(c)\n",
        "    if area > 40:\n",
        "        x,y,w,h = cv2.boundingRect(c)\n",
        "        cv2.rectangle(gan, (x, y), (x + w, y + h), (36,255,12), 2)\n",
        "        cv2.rectangle(reference, (x, y), (x + w, y + h), (36,255,12), 2)\n",
        "        cv2.drawContours(mask, [c], 0, (0,255,0), -1)\n",
        "        cv2.drawContours(filled_after, [c], 0, (0,255,0), -1)\n",
        "\n",
        "# Display images and differences using cv2_imshow\n",
        "cv2_imshow(gan)\n",
        "cv2_imshow(reference)\n",
        "cv2_imshow(diff)\n",
        "cv2_imshow(mask)\n",
        "cv2_imshow(filled_after)\n"
      ],
      "metadata": {
        "id": "VXErN8HmtzNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FID (Fréchet Inception Distance):\n",
        "\n",
        "FID compares the feature representations of real and generated images using a pre-trained deep learning model. It measures the similarity between the distributions of real and generated images in feature space.\n",
        "Lower FID values indicate better similarity between real and generated images."
      ],
      "metadata": {
        "id": "4EtCgPv0c691"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from scipy.linalg import sqrtm\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "def calculate_fid(real_images, generated_images):\n",
        "    # Load pre-trained InceptionV3 model\n",
        "    inception_model = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))\n",
        "\n",
        "    # Preprocess images\n",
        "    real_images = preprocess_input(real_images)\n",
        "    generated_images = preprocess_input(generated_images)\n",
        "\n",
        "    # Get features from Inception model\n",
        "    real_features = inception_model.predict(real_images)\n",
        "    generated_features = inception_model.predict(generated_images)\n",
        "\n",
        "    # Calculate mean and covariance of real and generated features\n",
        "    mu_real, sigma_real = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)\n",
        "    mu_generated, sigma_generated = np.mean(generated_features, axis=0), np.cov(generated_features, rowvar=False)\n",
        "\n",
        "    # Reshape covariance matrices to 2D\n",
        "    sigma_real = np.atleast_2d(sigma_real)\n",
        "    sigma_generated = np.atleast_2d(sigma_generated)\n",
        "\n",
        "    # Calculate Fréchet distance\n",
        "    epsilon = 1e-6\n",
        "    sqrt_sigma_real_generated = sqrtm(sigma_real @ sigma_generated)\n",
        "    fid = np.linalg.norm(mu_real - mu_generated) + np.trace(sigma_real + sigma_generated - 2 * sqrt_sigma_real_generated)\n",
        "\n",
        "    return fid, real_features, generated_features\n",
        "\n",
        "# Visualize t-SNE embeddings\n",
        "def visualize_tsne(real_features, generated_features):\n",
        "    all_features = np.concatenate([real_features, generated_features], axis=0)\n",
        "    all_labels = np.concatenate([np.ones(len(real_features)), np.zeros(len(generated_features))], axis=0)\n",
        "\n",
        "    # Choose a perplexity value less than the number of samples\n",
        "    perplexity = min(30, len(all_features) - 1)\n",
        "\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
        "    embedded_features = tsne.fit_transform(all_features)\n",
        "\n",
        "    plt.figure(figsize=(5, 3))\n",
        "    plt.scatter(embedded_features[all_labels == 1, 0], embedded_features[all_labels == 1, 1], c='blue', label='Real')\n",
        "    plt.scatter(embedded_features[all_labels == 0, 0], embedded_features[all_labels == 0, 1], c='red', label='Generated')\n",
        "    plt.title('t-SNE visualization of Inception features')\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Load and preprocess images\n",
        "real_image_path = '/content/160_img_.png' # reference image\n",
        "generated_image_path = '/content/gen_1.png' #generated image\n",
        "\n",
        "real_image = img_to_array(load_img(real_image_path, target_size=(299, 299)))\n",
        "generated_image = img_to_array(load_img(generated_image_path, target_size=(299, 299)))\n",
        "\n",
        "# Convert images to numpy arrays\n",
        "real_image = np.expand_dims(real_image, axis=0)\n",
        "generated_image = np.expand_dims(generated_image, axis=0)\n",
        "\n",
        "# Assume real_image and generated_image are numpy arrays of shape (1, height, width, channels)\n",
        "fid_score, real_features, generated_features = calculate_fid(real_image, generated_image)\n",
        "print(\"Fréchet Inception Distance (FID):\", fid_score)\n",
        "\n",
        "# Visualize t-SNE embeddings\n",
        "visualize_tsne(real_features, generated_features)"
      ],
      "metadata": {
        "id": "5Mw4Yv3Yc0tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F1-SCORE\n",
        "F1-Score is a measure of a test's accuracy that considers both the precision and recall of the test to compute the score. It is the harmonic mean of precision and recall.\n"
      ],
      "metadata": {
        "id": "6y9v_E8GdAox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess images\n",
        "real_image_path = '/content/160_img_.png' # reference image\n",
        "generated_image_path = '/content/gen_1.png' # generated image\n",
        "\n",
        "real_image = cv2.imread(real_image_path)\n",
        "generated_image = cv2.imread(generated_image_path)\n",
        "\n",
        "# Resize images to the same dimensions if needed\n",
        "real_image = cv2.resize(real_image, (generated_image.shape[1], generated_image.shape[0]))\n",
        "\n",
        "# Compute absolute pixel-wise difference between the generated and real images\n",
        "difference_image = cv2.absdiff(real_image, generated_image)\n",
        "\n",
        "# Apply a threshold to binarize the difference image\n",
        "threshold = 30\n",
        "_, binary_mask = cv2.threshold(cv2.cvtColor(difference_image, cv2.COLOR_BGR2GRAY), threshold, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# Flatten the binary masks for both the generated and real images\n",
        "flatten_generated_mask = binary_mask.flatten()  # Treat this as predicted labels\n",
        "flatten_real_mask = np.zeros_like(flatten_generated_mask)  # Treat this as true labels\n",
        "\n",
        "# Compute F1 score\n",
        "\n",
        "f1 = f1_score(flatten_real_mask, flatten_generated_mask, pos_label=0)\n",
        "\n",
        "print(\"F1 Score based on image difference:\", f1)\n",
        "\n"
      ],
      "metadata": {
        "id": "gO5COSULGFZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SSEQ  (Spatial-Spectral Entropy-based Quality):\n",
        "\n",
        "It computes the entropy of both spatial and spectral domains and calculates the index based on these values.It can be used to assess the fidelity and similarity of generated images compared to real images."
      ],
      "metadata": {
        "id": "6VaXraVPFGwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def compute_entropy(image):\n",
        "    # Convert image to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Compute histogram\n",
        "    hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n",
        "\n",
        "    # Normalize histogram\n",
        "    hist /= np.sum(hist)\n",
        "\n",
        "    # Compute entropy\n",
        "    entropy = -np.sum(hist * np.log2(hist + 1e-10))\n",
        "\n",
        "    return entropy\n",
        "\n",
        "def compute_spatial_spectral_entropy(real_image_path, generated_image_path):\n",
        "    # Load images\n",
        "    real_image = cv2.imread(real_image_path)\n",
        "    generated_image = cv2.imread(generated_image_path)\n",
        "\n",
        "    # Compute entropy for real and generated images\n",
        "    real_spatial_entropy = compute_entropy(real_image)\n",
        "    generated_spatial_entropy = compute_entropy(generated_image)\n",
        "\n",
        "    # Compute entropy for spectral domain\n",
        "    real_spectral_entropy = compute_entropy(real_image)\n",
        "    generated_spectral_entropy = compute_entropy(generated_image)\n",
        "\n",
        "    # Compute SSEQ index\n",
        "    sseq_index = (real_spatial_entropy / generated_spatial_entropy) * (real_spectral_entropy / generated_spectral_entropy)\n",
        "\n",
        "    return sseq_index\n",
        "\n",
        "# Example usage\n",
        "real_image_path = '/content/160_img_.png'\n",
        "generated_image_path = '/content/man_gen.png'\n",
        "sseq_index = compute_spatial_spectral_entropy(real_image_path, generated_image_path)\n",
        "print(\"Spatial-Spectral Entropy-based Quality (SSEQ) Index:\", sseq_index)\n"
      ],
      "metadata": {
        "id": "bqNUAWAQIUGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UIQM (Universal Image Quality Metric):\n",
        "\n",
        "UIQM is a universal image quality metric that considers various image quality factors, including sharpness, colorfulness, contrast, and exposure.\n",
        "It is suitable for assessing the overall quality of images in various applications."
      ],
      "metadata": {
        "id": "5kOppnhRFCkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def get_uiqm(image):\n",
        "    # Convert the image to LAB color space\n",
        "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "\n",
        "    # Calculate mean and standard deviation of each channel\n",
        "    mean_l, std_l = np.mean(lab_image[:,:,0]), np.std(lab_image[:,:,0])\n",
        "    mean_a, std_a = np.mean(lab_image[:,:,1]), np.std(lab_image[:,:,1])\n",
        "    mean_b, std_b = np.mean(lab_image[:,:,2]), np.std(lab_image[:,:,2])\n",
        "\n",
        "    # Calculate contrast\n",
        "    contrast = np.sqrt(std_l ** 2 + std_a ** 2 + std_b ** 2)\n",
        "\n",
        "    # Calculate saturation\n",
        "    saturation = np.sqrt(std_a ** 2 + std_b ** 2)\n",
        "\n",
        "    # Calculate average intensity\n",
        "    avg_intensity = np.mean(image)\n",
        "\n",
        "    # Calculate exposure\n",
        "    exposure = (np.mean(image) - 128) ** 2\n",
        "\n",
        "    # Calculate colorfulness\n",
        "    rg = image[:,:,0] - image[:,:,1]\n",
        "    yb = 0.5 * (image[:,:,0] + image[:,:,1]) - image[:,:,2]\n",
        "    colorfulness = np.sqrt(np.mean(rg ** 2 + yb ** 2))\n",
        "\n",
        "    # Calculate naturalness\n",
        "    naturalness = 0.486 * contrast + 0.319 * saturation + 0.115 * avg_intensity + 0.031 * exposure + 0.048 * colorfulness\n",
        "\n",
        "    return naturalness\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your underwater image\n",
        "    image = cv2.imread('/content/man_gen.png') #generated image\n",
        "\n",
        "    # Calculate UIQM\n",
        "    uiqm_score = get_uiqm(image)\n",
        "\n",
        "    print(\"UIQM Score:\", uiqm_score)\n"
      ],
      "metadata": {
        "id": "A8uM8It7_ypm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}